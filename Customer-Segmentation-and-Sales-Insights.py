# -*- coding: utf-8 -*-
"""chips_analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lIa4W2eMrbD3tp_FicW8_RfIHMiafRNx

# Import Required Libraries and Read Data
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
# %pip install numpy
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
from scipy.stats import ttest_ind

# Load data
transaction_data = pd.read_csv("QVI_transaction_data.csv")
customer_data = pd.read_csv("QVI_purchase_behaviour.csv")

"""# Exploratory Data Analysis"""

print("Transaction Data Structure:")
display(transaction_data.info())
display(transaction_data.head(10))

# Convert DATE column
transaction_data['DATE'] = pd.to_datetime(transaction_data['DATE'], origin='1899-12-30', unit='D')

"""# Product Analysis"""

# Extract unique words from PROD_NAME
product_words = pd.Series(" ".join(transaction_data['PROD_NAME'].unique()).split())
product_words = product_words[product_words.str.match("^[A-Za-z]+$")]

# Count frequency
word_counts = product_words.value_counts().reset_index()
word_counts.columns = ['words', 'count']
print(word_counts.head(10))

# Remove salsa products
transaction_data = transaction_data[~transaction_data['PROD_NAME'].str.lower().str.contains("salsa")]

"""# Handle Outlier"""

# Identify outlier
outlier_customer = transaction_data[transaction_data['PROD_QTY'] == 200]['LYLTY_CARD_NBR'].unique()
transaction_data = transaction_data[~transaction_data['LYLTY_CARD_NBR'].isin(outlier_customer)]

"""# Transactions Over Time"""

daily_transactions = transaction_data.groupby('DATE').size().reset_index(name='N')

# Fill missing dates
date_sequence = pd.date_range(start="2018-07-01", end="2019-06-30")
transaction_count_full = pd.DataFrame({'DATE': date_sequence})
transaction_count_full = transaction_count_full.merge(daily_transactions, on='DATE', how='left').fillna(0)

# Plot transactions over time
plt.figure(figsize=(14, 5))
sns.lineplot(data=transaction_count_full, x='DATE', y='N')
plt.title('Transactions Over Time')
plt.xticks(rotation=90)
plt.tight_layout()
plt.show()

# December plot
december_data = transaction_count_full[(transaction_count_full['DATE'] >= "2018-12-01") & (transaction_count_full['DATE'] <= "2018-12-31")]

plt.figure(figsize=(12, 4))
sns.lineplot(data=december_data, x='DATE', y='N')
plt.title('Transactions in December 2018')
plt.xticks(rotation=90)
plt.tight_layout()
plt.show()

"""# Extract Pack Size and Clean Brands"""

# Extract pack size
transaction_data['PACK_SIZE'] = transaction_data['PROD_NAME'].str.extract(r'(\d+)').astype(float)

# Histogram of PACK_SIZE
plt.figure(figsize=(10, 5))
sns.histplot(data=transaction_data, x='PACK_SIZE', bins=30)
plt.title('Number of Transactions by Pack Size')
plt.xticks(rotation=90)
plt.tight_layout()
plt.show()

# Extract brand (first word)
transaction_data['BRAND'] = transaction_data['PROD_NAME'].str.split().str[0].str.upper()

# Clean up brand names
brand_corrections = {
    "RED": "RRD", "SNBTS": "SUNBITES", "INFZNS": "INFUZIONS",
    "WW": "WOOLWORTHS", "SMITH": "SMITHS", "NCC": "NATURAL",
    "DORITO": "DORITOS", "GRAIN": "GRNWVES"
}
transaction_data['BRAND'] = transaction_data['BRAND'].replace(brand_corrections)

"""# Bar Plot for LIFESTAGE Distribution"""

# Set the plot style
plt.figure(figsize=(10, 6))
sns.countplot(data=customer_data, x='LIFESTAGE', color='blue', edgecolor='black', alpha=0.7)

# Customize plot
plt.title('LIFESTAGE Distribution')
plt.xlabel('Life Stage')
plt.ylabel('Count')
plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels
plt.tight_layout()
plt.show()

"""# Merge and Analyze Customer Data"""

data = pd.merge(transaction_data, customer_data, on="LYLTY_CARD_NBR", how="left")
missing_customers = data['LYLTY_CARD_NBR'].isna().sum()
print("Missing customer entries:", missing_customers)

# Save merged data
data.to_csv("QVI_data.csv", index=False)

"""# Sales Analysis and Visualization"""

# Total sales by segment
total_sales_by_segment = data.groupby(['LIFESTAGE', 'PREMIUM_CUSTOMER'])['TOT_SALES'].sum().reset_index()

# Pivot for stacked bar
pivot_data = total_sales_by_segment.pivot(index='LIFESTAGE', columns='PREMIUM_CUSTOMER', values='TOT_SALES')

# Normalize to get proportions
pivot_prop = pivot_data.div(pivot_data.sum(axis=1), axis=0)

# Plot
pivot_prop.plot(kind='bar', stacked=True, figsize=(12, 6), colormap='Set2')

plt.title('Proportion of Sales by Premium Status within Each Lifestage')
plt.ylabel('Proportion')
plt.xlabel('Lifestage')
plt.xticks(rotation=45, ha='right')
plt.legend(title='Premium Customer', bbox_to_anchor=(1.05, 1), loc='upper left')
plt.tight_layout()
plt.show()

"""# Count of Customers by LIFESTAGE and PREMIUM_CUSTOMER"""

# Count unique customers
customer_count_by_segment = (
    data.groupby(['LIFESTAGE', 'PREMIUM_CUSTOMER'])['LYLTY_CARD_NBR']
    .nunique()
    .reset_index(name='CUSTOMERS')
    .sort_values(by='CUSTOMERS', ascending=False)
)

# Plot
plt.figure(figsize=(12, 6))
sns.barplot(data=customer_count_by_segment, x='LIFESTAGE', y='CUSTOMERS', hue='PREMIUM_CUSTOMER')
plt.title('Number of Customers by Lifestage and Premium Customer Status')
plt.xlabel('Lifestage')
plt.ylabel('Number of Customers')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

"""# Average Number of Units per Customer"""

avg_units_per_customer = (
    data.groupby(['LIFESTAGE', 'PREMIUM_CUSTOMER'])
    .apply(lambda x: x['PROD_QTY'].sum() / x['LYLTY_CARD_NBR'].nunique())
    .reset_index(name='AVG')
    .sort_values(by='AVG', ascending=False)
)

# Plot
plt.figure(figsize=(12, 6))
sns.barplot(data=avg_units_per_customer, x='LIFESTAGE', y='AVG', hue='PREMIUM_CUSTOMER')
plt.title('Average Number of Units per Customer by Lifestage and Premium Customer Status')
plt.xlabel('Lifestage')
plt.ylabel('Average Number of Units')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

"""# Average Price per Unit"""

avg_price_per_unit = (
    data.groupby(['LIFESTAGE', 'PREMIUM_CUSTOMER'])
    .apply(lambda x: x['TOT_SALES'].sum() / x['PROD_QTY'].sum())
    .reset_index(name='AVG')
    .sort_values(by='AVG', ascending=False)
)

# Plot
plt.figure(figsize=(12, 6))
sns.barplot(data=avg_price_per_unit, x='LIFESTAGE', y='AVG', hue='PREMIUM_CUSTOMER')
plt.title('Average Price per Unit by Lifestage and Premium Customer Status')
plt.xlabel('Lifestage')
plt.ylabel('Average Price per Unit')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

"""# Perform Independent t-test"""

# Add price per unit column
data['price'] = data['TOT_SALES'] / data['PROD_QTY']

# Filter target groups
mainstream_group = data[
    (data['LIFESTAGE'].isin(['YOUNG SINGLES/COUPLES', 'MIDAGE SINGLES/COUPLES'])) &
    (data['PREMIUM_CUSTOMER'] == 'Mainstream')
]['price']

non_mainstream_group = data[
    (data['LIFESTAGE'].isin(['YOUNG SINGLES/COUPLES', 'MIDAGE SINGLES/COUPLES'])) &
    (data['PREMIUM_CUSTOMER'] != 'Mainstream')
]['price']

# Perform one-tailed t-test
t_stat, p_value = ttest_ind(mainstream_group, non_mainstream_group, alternative='greater')

# Print full precision
print("T-test result:")
print("t-statistic =", t_stat)
print("p-value =", p_value)

"""# Deep Dive into Mainstream Young Singles/Couples"""

segment1 = data[
    (data['LIFESTAGE'] == 'YOUNG SINGLES/COUPLES') &
    (data['PREMIUM_CUSTOMER'] == 'Mainstream')
]

other = data[~((data['LIFESTAGE'] == 'YOUNG SINGLES/COUPLES') &
               (data['PREMIUM_CUSTOMER'] == 'Mainstream'))]

"""# Brand Affinity Analysis"""

# Total quantity
quantity_segment1 = segment1['PROD_QTY'].sum()
quantity_other = other['PROD_QTY'].sum()

# Share by brand
quantity_segment1_by_brand = segment1.groupby('BRAND')['PROD_QTY'].sum().reset_index()
quantity_segment1_by_brand['targetSegment'] = quantity_segment1_by_brand['PROD_QTY'] / quantity_segment1
quantity_segment1_by_brand.drop('PROD_QTY', axis=1, inplace=True)

quantity_other_by_brand = other.groupby('BRAND')['PROD_QTY'].sum().reset_index()
quantity_other_by_brand['other'] = quantity_other_by_brand['PROD_QTY'] / quantity_other
quantity_other_by_brand.drop('PROD_QTY', axis=1, inplace=True)

# Merge and compute affinity
brand_proportions = pd.merge(quantity_segment1_by_brand, quantity_other_by_brand, on='BRAND')
brand_proportions['affinityToBrand'] = brand_proportions['targetSegment'] / brand_proportions['other']
brand_proportions = brand_proportions.sort_values(by='affinityToBrand', ascending=False)

print(brand_proportions[['BRAND', 'targetSegment', 'other', 'affinityToBrand']])

"""# Pack Size Affinity Analysis"""

# Share by pack size
quantity_segment1_by_pack = segment1.groupby('PACK_SIZE')['PROD_QTY'].sum().reset_index()
quantity_segment1_by_pack['targetSegment'] = quantity_segment1_by_pack['PROD_QTY'] / quantity_segment1
quantity_segment1_by_pack.drop('PROD_QTY', axis=1, inplace=True)

quantity_other_by_pack = other.groupby('PACK_SIZE')['PROD_QTY'].sum().reset_index()
quantity_other_by_pack['other'] = quantity_other_by_pack['PROD_QTY'] / quantity_other
quantity_other_by_pack.drop('PROD_QTY', axis=1, inplace=True)

# Merge and compute affinity
pack_proportions = pd.merge(quantity_segment1_by_pack, quantity_other_by_pack, on='PACK_SIZE')
pack_proportions['affinityToPack'] = pack_proportions['targetSegment'] / pack_proportions['other']
pack_proportions = pack_proportions.sort_values(by='affinityToPack', ascending=False)

print(pack_proportions[['PACK_SIZE', 'targetSegment', 'other', 'affinityToPack']])

"""# Conclusion

High sales volumes were driven by Budget older families, Mainstream young singles/couples, and Mainstream retirees.

Young singles/couples and mid-age groups are more likely to buy higher-priced or impulse chips.

Tyrrells and Twisties are more favored among young singles/couples.
"""